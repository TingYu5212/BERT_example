{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://www.iamzxs.com/2019/08/23/%E4%BD%BF%E7%94%A8bert%E6%A8%A1%E5%9E%8B%E7%94%9F%E6%88%90token%E7%BA%A7%E5%90%91%E9%87%8F/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\GitHub\\\\bert', 'C:\\\\Users\\\\User\\\\Anaconda3\\\\envs\\\\tf2.0\\\\python37.zip', 'C:\\\\Users\\\\User\\\\Anaconda3\\\\envs\\\\tf2.0\\\\DLLs', 'C:\\\\Users\\\\User\\\\Anaconda3\\\\envs\\\\tf2.0\\\\lib', 'C:\\\\Users\\\\User\\\\Anaconda3\\\\envs\\\\tf2.0', '', 'C:\\\\Users\\\\User\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages', 'C:\\\\Users\\\\User\\\\Anaconda3\\\\envs\\\\tf2.0\\\\lib\\\\site-packages', 'C:\\\\Users\\\\User\\\\Anaconda3\\\\envs\\\\tf2.0\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\User\\\\Anaconda3\\\\envs\\\\tf2.0\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\User\\\\Anaconda3\\\\envs\\\\tf2.0\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\User\\\\Anaconda3\\\\envs\\\\tf2.0\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\電子電路\\\\.ipython', 'D:\\\\GitHub']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "curPath = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "rootPath = os.path.split(curPath)[0]\n",
    "sys.path.append(rootPath)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install bert-for-tf2\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tokenization\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertModel, BertConfig\n",
    "#import modeling\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置文件\n",
    "# data_root是模型文件，可以用预训练的，也可以用在分类任务上微调过的模型\n",
    "data_root = './chinese_L-12_H-768_A-12/'\n",
    "bert_config_file = data_root + 'bert_config.json'\n",
    "bert_config = BertConfig.from_json_file(bert_config_file)\n",
    "init_checkpoint = data_root + 'bert_model.ckpt'\n",
    "bert_vocab_file = data_root + 'vocab.txt'\n",
    "# 经过处理的输入文件路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding存放路径\n",
    "emb_file_dir = './data/emb.h5'\n",
    "\n",
    "# graph\n",
    "#import tensorflow.compat.v1 as tf1\n",
    "#tf1.disable_v2_behavior()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "input_ids = tf.compat.v1.placeholder(tf.int32, shape=[None, None], name='input_ids')\n",
    "input_mask = tf.compat.v1.placeholder(tf.int32, shape=[None, None], name='input_masks')\n",
    "segment_ids = tf.compat.v1.placeholder(tf.int32, shape=[None, None], name='segment_ids')\n",
    "BATCH_SIZE = 16\n",
    "SEQ_LEN = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(x, batch_size, shuffle=False):\n",
    "    \"\"\"生成批次数据，一个batch一个batch地产生句子向量\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) + 1\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(np.arange(data_len))\n",
    "        x_shuffle = np.array(x)[indices]\n",
    "    else:\n",
    "        x_shuffle = x[:]\n",
    "    #word_mask = [[1] * (SEQ_LEN + 2) for i in range(data_len)]\n",
    "    word_mask = [[1] * (SEQ_LEN) for i in range(data_len)]\n",
    "    word_segment_ids = [[0] * (SEQ_LEN) for i in range(data_len)]\n",
    "    #word_segment_ids = [[0] * (SEQ_LEN+2) for i in range(data_len)]]\n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], word_mask[start_id:end_id], word_segment_ids[start_id:end_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(file_dir):\n",
    "    # 从文件中读到所有需要转化的句子\n",
    "    # 这里需要做统一长度为510\n",
    "    input_list = []\n",
    "    df = pd.read_csv(file_dir)\n",
    "    #print(type(df_sentence))\n",
    "    for i in df[\"sentence\"]:\n",
    "        input_list.append(i)\n",
    "    # input_list是输入list，每一个元素是一个str，代表输入文本\n",
    "    # 现在需要转化成id_list\n",
    "    word_id_list = []\n",
    "    for query in input_list:\n",
    "        split_tokens = token.tokenize(query)\n",
    "        if len(split_tokens) > SEQ_LEN:\n",
    "            split_tokens = split_tokens[:SEQ_LEN]\n",
    "        else:\n",
    "            while len(split_tokens) < SEQ_LEN:\n",
    "                split_tokens.append('[PAD]')\n",
    "        # ****************************************************\n",
    "        # 如果是需要用到句向量，需要用这个方法\n",
    "        # 加个CLS头，加个SEP尾\n",
    "        tokens = []\n",
    "        #tokens.append(\"[CLS]\")\n",
    "        for i_token in split_tokens:\n",
    "            tokens.append(i_token)\n",
    "        #tokens.append(\"[SEP]\")\n",
    "        # ****************************************************\n",
    "        word_ids = token.convert_tokens_to_ids(tokens)\n",
    "        word_id_list.append(word_ids)\n",
    "    return word_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 初始化BERT\n",
    "import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From D:\\GitHub\\bert\\modeling.py:700: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/google-research/bert/pull/876/files\n",
    "import tensorflow as tf\n",
    "model = modeling.BertModel(\n",
    "    config=bert_config,\n",
    "    is_training=False,\n",
    "    input_ids=input_ids,\n",
    "    input_mask=input_mask,\n",
    "    token_type_ids=segment_ids,\n",
    "    use_one_hot_embeddings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载BERT模型\n",
    "tvars = tf.compat.v1.trainable_variables()\n",
    "(assignment, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取最后一层和倒数第二层\n",
    "encoder_last_layer = model.get_sequence_output()\n",
    "encoder_last2_layer = model.all_encoder_layers[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(760, 25, 768)\n",
      "(160, 25, 768)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "#token = tokenization.tokenizer(vocab_file=bert_vocab_file)\n",
    "bert_vocab_file = './chinese_L-12_H-768_A-12/vocab.txt'\n",
    "token = BertTokenizer(vocab_file=bert_vocab_file)\n",
    "input_train_data = read_input(file_dir='train134.csv')\n",
    "#input_val_data = read_input(file_dir='../data/legal_domain/val_x_c.txt')\n",
    "input_test_data = read_input(file_dir='test134.csv')\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    save_file = h5py.File('./downstream1/input_c_emb.h5', 'w')\n",
    "    emb_train = []\n",
    "    train_batches = batch_iter(input_train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    for word_id, mask, segment in train_batches:\n",
    "        feed_data = {input_ids: word_id, input_mask: mask, segment_ids: segment}\n",
    "\n",
    "        last2 = sess.run(encoder_last_layer, feed_dict=feed_data)\n",
    "        # print(last2.shape)\n",
    "        for sub_array in last2:\n",
    "            emb_train.append(sub_array)\n",
    "    # 可以保存了\n",
    "    emb_train_array = np.asarray(emb_train)\n",
    "    save_file.create_dataset('train', data=emb_train_array)\n",
    "    \n",
    "    # test\n",
    "    emb_test = []\n",
    "    test_batches = batch_iter(input_test_data, BATCH_SIZE, shuffle=False)\n",
    "    for word_id, mask, segment in test_batches:\n",
    "        feed_data = {input_ids: word_id, input_mask: mask, segment_ids: segment}\n",
    "        last2 = sess.run(encoder_last_layer, feed_dict=feed_data)\n",
    "        # print(last2.shape)\n",
    "        for sub_array in last2:\n",
    "            emb_test.append(sub_array)\n",
    "    # 可以保存了\n",
    "    emb_test_array = np.asarray(emb_test)\n",
    "    save_file.create_dataset('test', data=emb_test_array)\n",
    "    save_file.close()\n",
    "    print(emb_train_array.shape)\n",
    "    print(emb_test_array.shape)\n",
    "\n",
    "    # 这边目标是接下游CNN任务，因此先写入所有token的embedding，768维\n",
    "    # 写入shape直接是(N, max_seq_len + 2, 768)\n",
    "    # 下游需要选用的时候，如果卷积，则去掉头尾使用，如果全连接，则直接使用头部\n",
    "    # 这里直接设定max_seq_len=510，加上[cls]和[sep]，得到512\n",
    "    # 写入(n, 512, 768) ndarray到文件，需要用的时候再读出来，就直接舍弃embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-62204ea782e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 activation='sigmoid' )\n\u001b[0;32m      8\u001b[0m ])\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m model.compile(\n\u001b[0;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     \"\"\"\n\u001b[0;32m   1256\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1257\u001b[1;33m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[0;32m   1258\u001b[0m                        \u001b[1;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m                        \u001b[1;34m'`fit()` with some data, or specify '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model8 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(512,return_sequences=True),\n",
    "    tf.keras.layers.LSTM(128,return_sequences=False),\n",
    "    tf.keras.layers.Dense(64,\n",
    "                activation='sigmoid' ),\n",
    "    tf.keras.layers.Dense(2,\n",
    "                activation='sigmoid' )\n",
    "])\n",
    "model8.summary()\n",
    "model.compile(\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "loss=tf.keras.losses.sparse_binary_crossentropy,\n",
    "metrics=[tf.keras.metrics.sparse_binary_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
